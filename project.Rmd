---
title: "project"
author: "Chenyang Nie"
date: "6/1/2020"
output: html_document
---
# functions
```{r}
# # # log.sum # # #
log.sum <- function(log.a,log.b){
# sum of logarithms log(a+b)
  larger = max(c(log.a,log.b))
  smaller = min(c(log.a,log.b))
  res = larger + log(1 + exp(smaller - larger))
  return(res)
}

# # # compute.likelihood # # #
compute.likelihood <- function(doc, model, phi, gammav){
  # compute likelihood according to equation (15) in Blei's LDA paper
  likelihood = 0
  alpha = model$alpha
  nTopics = model$ntopics
  dig = sapply(gammav,digamma)
  gammav.sum = sum(gammav)
  digsum = digamma(gammav.sum)
  likelihood = lgamma(alpha*nTopics) - nTopics*lgamma(alpha) - lgamma(gammav.sum)
  # print(likelihood)
  for (k in 1:nTopics){
    addlike = (alpha - 1)*(dig[k] - digsum) + lgamma(gammav[k]) - (gammav[k] - 1)*(dig[k] - digsum)
    likelihood = likelihood + addlike
    # print(sprintf("k_num %f",addlike))
    for (n in 1:doc$dlength){
      if (phi[n,k] > 0){
        addlike = doc$counts[n]*(phi[n,k]*((dig[k] - digsum) - log(phi[n,k]) + model$logProbW[k,doc$words[n]+1]))
        # print(sprintf("kn_num %f",addlike))
        likelihood = likelihood + addlike
      }
    }
  }
  # print(likelihood)
  return(likelihood)
}


mstep.beta <- function(ldamodel,sstats){
# estimate beta (logProbW) according to equation (7) of C.Reed's tutorial
  
  for (k in 1:ldamodel$ntopics){
    for (w in 1:ldamodel$nterms){
      if (sstats$classword[k,w] > 0 ){
        ldamodel$logProbW[k,w] = log(sstats$classword[k,w]) - log(sstats$classtotal[k])
      }
      else{
        ldamodel$logProbW[k,w] = -100
      }
    }
  }
  return(ldamodel)
}
```

``` {r}
corpus1 = read.table('/Users/nick/Documents/Fintech/stats ML/project/LDA_datasets/text_data.txt',head=TRUE)

corpus1_list<-list()
corpus1_list$nterms<-100
corpus1_list$ndocs<-300
docs<-list()

for (i in 1:nrow(corpus1)) {
        docs[[i]] <- list()
        # docs[[i]]$words <- colnames(corpus1)[which(corpus1[i,]>0)]
        docs[[i]]$words <- match(colnames(corpus1)[which(corpus1[i,]>0)],names(corpus1))-1
        docs[[i]]$counts <- corpus1[i,][corpus1[i,] > 0]
        docs[[i]]$dlength <- length(corpus1[i,][corpus1[i,] > 0])
        docs[[i]]$total <- sum(corpus1[i,][corpus1[i,] > 0])
}
corpus1_list$docs<-docs

corpus<-corpus1_list

k=3 # number of topics
estAlpha = TRUE
MAX.ES.IT = 20
ES.CONV.LIM = 1e-6
EM.CONV = 1e-3
MAX.EM = 50
alpha = 50/k;

# init the model randomly 
cwinit = matrix(runif(k*corpus$nterms),k,corpus$nterms) + 1/corpus$nterms
ldamodel = list(logProbW=matrix(rep(0,k*corpus$nterms),k,corpus$nterms), alpha = 1, ntopics=k, nterms=corpus$nterms)
sstats = list(ndocs=0,classword=cwinit,k,corpus$nterms,classtotal=rowSums(cwinit), alpha.ss = 0)
ldamodel = mstep.beta(ldamodel,sstats)
ldamodel$alpha = alpha 

like.hist = c() # keep track of the likelihood
likelihood.prev = 0
numit = 0
hasConverged = FALSE
nTopics = ldamodel$ntopics
nTerms = ldamodel$nterms
phi = matrix(rep(0,nTopics*nTerms),nTopics, nTerms)

# # # # Run variational expectation-maximization # # # #
while (!hasConverged){
        numit = numit + 1
        print(sprintf("----- EM Iteration %i ----- ", numit))
        
        # reset sufficient statistics and likelihood
        sstats$classword = matrix(rep(0,nTopics*nTerms), nrow=nTopics, nTerms) 
        sstats$classtotal = rep(0,nTopics)
        sstats$ndocs = 0
        sstats$alpha.ss = 0
        likelihood = 0
        gamma_topic <- matrix(rep(0,900),nrow = 3, ncol = 300)
        
        # # # do E-step # # #
        for (d in 1:corpus$ndocs){
                if (d %% 100 == 0){
                        print(sprintf("~~ completed e-step for %i docs ~~",d))
                } 
                
                # # do posterior inference # # 
                
                # initialize the document specific variables
                doc.oldlike = 0
                doc.length = corpus$docs[[d]]$dlength
                doc.totlen = corpus$docs[[d]]$total
                gammav = rep(ldamodel$alpha + doc.totlen/nTopics, nTopics)
                digamma.gam = rep(digamma(ldamodel$alpha + doc.totlen/nTopics), nTopics)
                phi = matrix(rep(1/nTopics, doc.length*nTopics), nrow=doc.length, ncol=nTopics)
                oldphi = phi[1,]
                
                # compute posterior dirichlet
                estep.converged = FALSE
                numits.es = 0;
                while (!estep.converged){
                        numits.es = numits.es + 1
                        # TODO: rewrite "R-style"
                        for (n in 1:doc.length){
                                phisum = 0
                                for (k in 1:nTopics){
                                        oldphi[k] = phi[n,k]
                                        phi[n,k] = digamma.gam[k] + ldamodel$logProbW[k, corpus$docs[[d]]$words[[n]]+1]
                                        
                                        if (k > 1){
                                                phisum = log.sum(phisum,phi[n,k])
                                        }
                                        else{
                                                phisum = phi[n,k]
                                        }
                                }
                                
                                for (k in 1:nTopics){
                                        phi[n,k] = exp(phi[n,k] - phisum)
                                        gammav[k] = gammav[k] + corpus$docs[[d]]$counts[[n]]*(phi[n,k] - oldphi[k])
                                        digamma.gam[k] = digamma(gammav[k])
                                        if (is.na(gammav[k])){
                                                print('error with gammav, contains na')
                                                browser()
                                        }
                                }
                        }
                        
                        # determine if the documents likelihood has converged
                        doc.like = compute.likelihood(corpus$docs[[d]], ldamodel, phi, gammav)
                        convfrac = (doc.oldlike - doc.like) / doc.oldlike
                        doc.oldlike = doc.like
                        
                        
                        if (convfrac < ES.CONV.LIM || numits.es > MAX.ES.IT){
                                estep.converged = TRUE
                                # print(sprintf("leaving E-step after %i iterations and convfrac: %1.3e, doc-likelihood: %1.3e", numits.es, convfrac, doc.like))
                                # plot(doc.histlike)
                        }
                } # end while e-step has not converged
                
                # # update the sufficient statistics for the M-step # #
                gamma.sum = sum(gammav)
                sstats$alpha.ss = sstats$alpha.ss + sum(sapply(gammav,digamma))
                sstats$alpha.ss = sstats$alpha.ss - nTopics*digamma(gamma.sum)
                
                for (n in 1:doc.length ){
                        for (k in 1:nTopics){
                                phink = corpus$docs[[d]]$counts[n]*phi[n,k]
                                sstats$classword[k,corpus$docs[[d]]$words[n] + 1] = sstats$classword[k,corpus$docs[[d]]$words[n] + 1] + phink
                                sstats$classtotal[k] = sstats$classtotal[k] + phink
                        }
                }
                sstats$ndocs = sstats$ndocs + 1
                likelihood = likelihood + doc.like
                
                gamma_topic[1,d] <- gammav[1]/sum(gammav)
                gamma_topic[2,d] <- gammav[2]/sum(gammav)
                gamma_topic[3,d] <- gammav[3]/sum(gammav)
                
        } # end for each document
        
        # # # do M-step # # #
        
        print("[doing m-step]")
        
        # estimate beta
        ldamodel = mstep.beta(ldamodel,sstats)
        
        # estimate alpha
        if (estAlpha){
                D = sstats$ndocs
                alpha.iter = 0
                a.init = 100
                log.a = log(a.init)
                alpha.hasconv = FALSE
                while (!alpha.hasconv){
                        alpha.iter = alpha.iter + 1
                        a = exp(log.a)
                        
                        if (is.nan(a)){
                                a.init = a.init*10
                                print(sprintf("alpha became nan, initializing with alpha = %1.3e",a.init))
                                a = a.init
                                log.a = log(a)
                        }
                        
                        f = D*(lgamma(nTopics*a) - nTopics*lgamma(a)) + (a-1)*sstats$alpha.ss
                        df = D * (nTopics*digamma(nTopics*a) - nTopics*digamma(a)) + sstats$alpha.ss
                        d2f = D * (nTopics*nTopics*trigamma(nTopics*a) - nTopics*trigamma(a))
                        log.a = log.a - df/(d2f*a + df)
                        print(sprintf("alpha optimization: %1.3e  %1.3e   %1.3e", exp(log.a), f, df))
                        if (abs(df) < 1e-5 || alpha.iter > 100){
                                alpha.hasconv = TRUE
                        }
                }
                ldamodel$alpha = exp(log.a)
        }
        
        conv.em = (likelihood.prev - likelihood)/likelihood.prev
        likelihood.prev = likelihood
        like.hist[numit] = likelihood
        
        # make sure we're iterating enough for the likelihood to converge'
        if (conv.em < 0){ 
                MAX.ES.IT = MAX.ES.IT*2
        }
        if (((conv.em < EM.CONV && conv.em > 0)  || numit > MAX.EM) && numit > 2){
                print(sprintf("Converged with conv = %0.3f and %i iterations",conv.em,numit))
                hasConverged = TRUE
        }
        print(sprintf("likelihood: %1.4e, conv: %1.4e",likelihood, conv.em))
        plot(like.hist)
}
print("----- Finished -----")
```


``` {r}
pred<-t(gamma_topic)
pred_sort<-t(apply(pred,1,function(x){sort(x)}))
colnames(pred_sort)<-c('pred_T1','pred_T2','pred_T3')
```

# predicted result and true result comparison. we can see the topic distribution in 300 documents are close among true and predicted.
``` {r}
topic_true = read.table('/Users/nick/Documents/Fintech/stats ML/project/LDA_datasets/topic_doc_true.txt',head=FALSE)
topic_true_sort<-t(apply(topic_true,2,function(x){sort(x)}))
colnames(topic_true_sort)<-c('true_T1','true_T2','true_T3')
cbind(topic_true_sort,pred_sort)
```