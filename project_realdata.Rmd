---
title: "project_realdata"
author: "Chenyang Nie"
date: "6/2/2020"
output: html_document
---
# code repo:  https://github.com/rushoj/ML-project
# functions
```{r}
# # # log.sum # # #
log.sum <- function(log.a,log.b){
# sum of logarithms log(a+b)
  larger = max(c(log.a,log.b))
  smaller = min(c(log.a,log.b))
  res = larger + log(1 + exp(smaller - larger))
  return(res)
}

# # # compute.likelihood # # #
compute.likelihood <- function(doc, model, phi, gammav){
  # compute likelihood according to equation (15) in Blei's LDA paper
  likelihood = 0
  alpha = model$alpha
  nTopics = model$ntopics
  dig = sapply(gammav,digamma)
  gammav.sum = sum(gammav)
  digsum = digamma(gammav.sum)
  likelihood = lgamma(alpha*nTopics) - nTopics*lgamma(alpha) - lgamma(gammav.sum)
  # print(likelihood)
  for (k in 1:nTopics){
    addlike = (alpha - 1)*(dig[k] - digsum) + lgamma(gammav[k]) - (gammav[k] - 1)*(dig[k] - digsum)
    likelihood = likelihood + addlike
    # print(sprintf("k_num %f",addlike))
    for (n in 1:doc$dlength){
      if (phi[n,k] > 0){
        addlike = doc$counts[n]*(phi[n,k]*((dig[k] - digsum) - log(phi[n,k]) + model$logProbW[k,doc$words[n]+1]))
        # print(sprintf("kn_num %f",addlike))
        likelihood = likelihood + addlike
      }
    }
  }
  # print(likelihood)
  return(likelihood)
}


mstep.beta <- function(ldamodel,sstats){
# estimate beta (logProbW) according to equation (7) of C.Reed's tutorial
  
  for (k in 1:ldamodel$ntopics){
    for (w in 1:ldamodel$nterms){
      if (sstats$classword[k,w] > 0 ){
        ldamodel$logProbW[k,w] = log(sstats$classword[k,w]) - log(sstats$classtotal[k])
      }
      else{
        ldamodel$logProbW[k,w] = -100
      }
    }
  }
  return(ldamodel)
}
```


```{r}
corpus1 = read.table('/Users/nick/Documents/Fintech/stats ML/project/LDA_datasets/cora_data.txt',head=TRUE)

corpus1_list<-list()
corpus1_list$nterms<-ncol(corpus1)
corpus1_list$ndocs<-nrow(corpus1)
docs<-list()

for (i in 1:nrow(corpus1)) {
  docs[[i]] <- list()
  # docs[[i]]$words <- colnames(corpus1)[which(corpus1[i,]>0)]
  docs[[i]]$words <- match(colnames(corpus1)[which(corpus1[i,]>0)],names(corpus1))-1
  docs[[i]]$counts <- corpus1[i,][corpus1[i,] > 0]
  docs[[i]]$dlength <- length(corpus1[i,][corpus1[i,] > 0])
  docs[[i]]$total <- sum(corpus1[i,][corpus1[i,] > 0])
}
corpus1_list$docs<-docs

corpus<-corpus1_list
```

```{r}
k=3 # number of topics
# other parameters
estAlpha = TRUE
MAX.ES.IT = 15
ES.CONV.LIM = 1e-7
EM.CONV = 1e-4
MAX.EM = 60
alpha = 2/k;
# alpha = 2/k;
# init the model randomly 
cwinit = matrix(runif(k*corpus$nterms),k,corpus$nterms) + 1/corpus$nterms
ldamodel = list(logProbW=matrix(rep(0,k*corpus$nterms),k,corpus$nterms), alpha = 1, ntopics=k, nterms=corpus$nterms)
sstats = list(ndocs=0,classword=cwinit,k,corpus$nterms,classtotal=rowSums(cwinit), alpha.ss = 0)
ldamodel = mstep.beta(ldamodel,sstats)
ldamodel$alpha = alpha 

like.hist = c() # keep track of the likelihood
likelihood.prev = 0
numit = 0
hasConverged = FALSE
nTopics = ldamodel$ntopics
nTerms = ldamodel$nterms
phi = matrix(rep(0,nTopics*nTerms),nTopics, nTerms)
```

# Run variational expectation-maximization
# Please stop and run again, 1st time running time is very long, but stop and rerun will be very fast.
```{r}
while (!hasConverged){
  numit = numit + 1
  print(sprintf("----- EM Iteration %i ----- ", numit))
  
  # reset sufficient statistics and likelihood
  sstats$classword = matrix(rep(0,nTopics*nTerms), nrow=nTopics, nTerms) 
  sstats$classtotal = rep(0,nTopics)
  sstats$ndocs = 0
  sstats$alpha.ss = 0
  likelihood = 0
  gamma_topic <- matrix(rep(0,k*ncol(corpus1)),nrow = k, ncol = ncol(corpus1))
  # # # do E-step # # #
  for (d in 1:corpus$ndocs){
    if (d %% 300 == 0){
      print(sprintf("~~ completed e-step for %i docs ~~",d))
    } 
    
    # # do posterior inference # # 
    
    # initialize the document specific variables
    doc.oldlike = 0
    doc.length = corpus$docs[[d]]$dlength
    doc.totlen = corpus$docs[[d]]$total
    gammav = rep(ldamodel$alpha + doc.totlen/nTopics, nTopics)
    digamma.gam = rep(digamma(ldamodel$alpha + doc.totlen/nTopics), nTopics)
    phi = matrix(rep(1/nTopics, doc.length*nTopics), nrow=doc.length, ncol=nTopics)
    oldphi = phi[1,]
    
    # compute posterior dirichlet
    estep.converged = FALSE
    numits.es = 0;
    while (!estep.converged){
      numits.es = numits.es + 1
      # TODO: rewrite "R-style"
      for (n in 1:doc.length){
        phisum = 0
        for (k in 1:nTopics){
          oldphi[k] = phi[n,k]
          phi[n,k] = digamma.gam[k] + ldamodel$logProbW[k, corpus$docs[[d]]$words[[n]]+1]
          
          if (k > 1){
            phisum = log.sum(phisum,phi[n,k])
          }
          else{
            phisum = phi[n,k]
          }
        }
        
        for (k in 1:nTopics){
          phi[n,k] = exp(phi[n,k] - phisum)
          gammav[k] = gammav[k] + corpus$docs[[d]]$counts[[n]]*(phi[n,k] - oldphi[k])
          digamma.gam[k] = digamma(gammav[k])
          if (is.na(gammav[k])){
            print('error with gammav, contains na')
            browser()
          }
        }
      }
      
      # determine if the documents likelihood has converged
      doc.like = compute.likelihood(corpus$docs[[d]], ldamodel, phi, gammav)
      convfrac = (doc.oldlike - doc.like) / doc.oldlike
      doc.oldlike = doc.like
      
      
      if (convfrac < ES.CONV.LIM || numits.es > MAX.ES.IT){
        estep.converged = TRUE
        # print(sprintf("leaving E-step after %i iterations and convfrac: %1.3e, doc-likelihood: %1.3e", numits.es, convfrac, doc.like))
        # plot(doc.histlike)
      }
    } # end while e-step has not converged
    
    # # update the sufficient statistics for the M-step # #
    gamma.sum = sum(gammav)
    sstats$alpha.ss = sstats$alpha.ss + sum(sapply(gammav,digamma))
    sstats$alpha.ss = sstats$alpha.ss - nTopics*digamma(gamma.sum)
    
    for (n in 1:doc.length ){
      for (k in 1:nTopics){
        phink = corpus$docs[[d]]$counts[n]*phi[n,k]
        sstats$classword[k,corpus$docs[[d]]$words[n] + 1] = sstats$classword[k,corpus$docs[[d]]$words[n] + 1] + phink
        sstats$classtotal[k] = sstats$classtotal[k] + phink
      }
    }
    sstats$ndocs = sstats$ndocs + 1
    likelihood = likelihood + doc.like
    gamma_topic[1,d] <- gammav[1]/sum(gammav)
    gamma_topic[2,d] <- gammav[2]/sum(gammav)
    gamma_topic[3,d] <- gammav[3]/sum(gammav)
    # gamma_topic[4,d] <- gammav[4]/sum(gammav)
    # gamma_topic[5,d] <- gammav[5]/sum(gammav)
    # gamma_topic[6,d] <- gammav[6]/sum(gammav)
    # gamma_topic[7,d] <- gammav[7]/sum(gammav)
    # gamma_topic[8,d] <- gammav[8]/sum(gammav)

  } # end for each document
  
  # # # do M-step # # #
  
  print("[doing m-step]")
  
  # estimate beta
  ldamodel = mstep.beta(ldamodel,sstats)
  
  # estimate alpha
  if (estAlpha){
    D = sstats$ndocs
    alpha.iter = 0
    a.init = 100
    log.a = log(a.init)
    alpha.hasconv = FALSE
    while (!alpha.hasconv){
      alpha.iter = alpha.iter + 1
      a = exp(log.a)
      
      if (is.nan(a)){
        a.init = a.init*10
        print(sprintf("alpha became nan, initializing with alpha = %1.3e",a.init))
        a = a.init
        log.a = log(a)
      }
      
      f = D*(lgamma(nTopics*a) - nTopics*lgamma(a)) + (a-1)*sstats$alpha.ss
      df = D * (nTopics*digamma(nTopics*a) - nTopics*digamma(a)) + sstats$alpha.ss
      d2f = D * (nTopics*nTopics*trigamma(nTopics*a) - nTopics*trigamma(a))
      log.a = log.a - df/(d2f*a + df)
      print(sprintf("alpha optimization: %1.3e  %1.3e   %1.3e", exp(log.a), f, df))
      if (abs(df) < 1e-5 || alpha.iter > 100){
        alpha.hasconv = TRUE
      }
    }
    ldamodel$alpha = exp(log.a)
  }
  
  conv.em = (likelihood.prev - likelihood)/likelihood.prev
  likelihood.prev = likelihood
  like.hist[numit] = likelihood
  
  # make sure we're iterating enough for the likelihood to converge'
  if (conv.em < 0){ 
    MAX.ES.IT = MAX.ES.IT*2
  }
  if (((conv.em < EM.CONV && conv.em > 0)  || numit > MAX.EM) && numit > 2){
    print(sprintf("Converged with conv = %0.3f and %i iterations",conv.em,numit))
    hasConverged = TRUE
  }
  print(sprintf("likelihood: %1.4e, conv: %1.4e",likelihood, conv.em))
  plot(like.hist)
}
print("----- Finished -----")
```

# topic distribution in first 20 documents
```{r}
head(t(gamma_topic),20)
```

# top 20 words in 3 topics, based on the top words, the potential topics could be deep learning, machine learning and academic paper
```{r}
indata = names(corpus1)
nwords=20
res = matrix(rep(0,ldamodel$ntopics*nwords), nrow=nwords, ncol=ldamodel$ntopics)
prbs = matrix(rep(0,ldamodel$ntopics*nwords), nrow=nwords, ncol=ldamodel$ntopics)
for (i in 1:ldamodel$ntopics){
  tmp = sort(ldamodel$logProbW[i,], decreasing=TRUE, index.return = TRUE)
  res[,i] = tmp$ix[1:nwords]
  prbs[,i] = tmp$x[1:nwords]
}

ldasumm = list()
ldasumm$words = matrix(indata[res], nrow=nwords, ncol=ldamodel$ntopics)
ldasumm$words
```






